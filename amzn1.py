# -*- coding: utf-8 -*-
"""AMZN1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m6savwFg_Ze1V_Wkx3hhN1iTOveoe3Cg
"""

import numpy as np

pip install yfinance pandas scikit-learn

import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import yfinance as yf

# Download historical stock data for Amazon (AMZN)
stock = yf.download("AMZN", start="2020-01-01", end="2024-03-01")

# Display the first few rows to confirm
print(stock.head())

# Reset index to make the date a column (optional)
stock.reset_index(inplace=True)

# Select relevant columns
df = stock[['Open', 'High', 'Low', 'Volume', 'Close']]

# Check for missing values
print(df.isnull().sum())
df = df.dropna()  # Drop rows with missing data

# Display the structure of the dataset
print(df.info())

# Features and target
X = df[['Open', 'High', 'Low', 'Volume']]
y = df['Close']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Evaluate on training data
y_train_pred = model.predict(X_train_scaled)
train_r2 = r2_score(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)

# Evaluate on testing data
y_test_pred = model.predict(X_test_scaled)
test_r2 = r2_score(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

# Print results
print("\nTraining Results:")
print(f"R²: {train_r2:.4f}")
print(f"MSE: {train_mse:.4f}")

print("\nTesting Results:")
print(f"R²: {test_r2:.4f}")
print(f"MSE: {test_mse:.4f}")

import matplotlib.pyplot as plt

# Residual plot for training data
train_residuals = y_train - y_train_pred
plt.scatter(y_train_pred, train_residuals, alpha=0.5)
plt.axhline(y=0, color='red', linestyle='--')
plt.title("Training Residuals")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()

# Residual plot for testing data
test_residuals = y_test - y_test_pred
plt.scatter(y_test_pred, test_residuals, alpha=0.5)
plt.axhline(y=0, color='red', linestyle='--')
plt.title("Testing Residuals")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()

import yfinance as yf
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd

# Fetch historical stock data (Example: Amazon)
stock = yf.download("AMZN", start="2020-01-01", end="2024-03-01")
df = stock.reset_index()

# Features and target
X = df[['Open', 'High', 'Low', 'Volume']]
y = df['Close']

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Evaluate the model
y_test_pred = model.predict(X_test_scaled)

# Example: Predict the close price for the last day in the dataset
new_data = X.iloc[-1].values.reshape(1, -1)  # Take the last row of features
new_data_scaled = scaler.transform(new_data)
predicted_close = model.predict(new_data_scaled)[0]

# Actual close price for the last day
actual_close = y.iloc[-1]  # Extract as scalar

# Compare predicted and actual close price
print(f"Actual Close Price: {actual_close}")
print(f"Predicted Close Price: {predicted_close}")

import pandas as pd
import numpy as np

# Flatten coefficients if needed
coefficients = np.ravel(model.coef_)

# Create the feature importance DataFrame
feature_importance = pd.DataFrame({
    'Feature': ['Open', 'High', 'Low', 'Volume'],
    'Coefficient': coefficients
})

print(feature_importance)

# Choose a specific row for testing (e.g., 100th row in the dataset)
# Choose a specific row for testing (e.g., 100th row in the dataset)
row_index = 100  # Change this to test different rows
new_data = X.iloc[row_index].values.reshape(1, -1)  # Select the row of features
new_data_scaled = scaler.transform(new_data)
predicted_close = float(model.predict(new_data_scaled)[0])  # Extract scalar from array

# Actual close price for the selected row
actual_close = float(y.iloc[row_index])  # Ensure it's a scalar

# Calculate the difference
difference = abs(float(actual_close) - float(predicted_close))  # Convert to float for safety

# Print the results
print(f"Row Index: {row_index}")
print(f"Actual Close Price: {float(actual_close):.4f}")  # Convert to float for formatting
print(f"Predicted Close Price: {float(predicted_close):.4f}")  # Convert to float for formatting
print(f"Difference: {difference:.4f}")



# Example input data
new_data = [[100, 105, 95, 50000000]]  # Replace with actual values
new_data_scaled = scaler.transform(new_data)
prediction = model.predict(new_data_scaled)
print(f"Predicted Close Price: {prediction[0]}")

print(f"y_test shape: {y_test.shape}")
print(f"y_test_pred shape: {y_test_pred.shape}")

y_test_pred = y_test_pred.ravel()
print(f"Flattened y_test_pred shape: {y_test_pred.shape}")

# Check if all samples in X_test are being processed
if len(y_test_pred) != len(y_test):
    print(f"Mismatch detected: y_test has {len(y_test)} samples, but y_test_pred has {len(y_test_pred)} samples.")

min_samples = min(len(y_test), len(y_test_pred))
y_test = y_test[:min_samples]
y_test_pred = y_test_pred[:min_samples]

# Compute metrics
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_test_pred)
print(f"Mean Absolute Error: {mae:.4f}")

from sklearn.metrics import mean_absolute_error

# Mean Absolute Error
mae = mean_absolute_error(y_test, y_test_pred)

# Print metrics
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {test_mse:.4f}")  # Already computed
print(f"R² Score: {test_r2:.4f}")  # Already computed

import pandas as pd
import numpy as np

# Assuming `df` contains your stock data
# Ensure `df` has a 'Close' column

# Simple Moving Average (SMA)
df['SMA_5'] = df['Close'].rolling(window=5).mean()
df['SMA_10'] = df['Close'].rolling(window=10).mean()

# Exponential Moving Average (EMA)
df['EMA_5'] = df['Close'].ewm(span=5, adjust=False).mean()
df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()

# Relative Strength Index (RSI)
def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df['RSI_14'] = calculate_rsi(df['Close'])

# Drop rows with NaN values (from rolling calculations)
df.dropna(inplace=True)

print(df.head())  # Verify the new columns

import numpy as np

def create_sliding_window(data, input_days=5, output_days=1):
    """
    Create input-output pairs for sliding window approach.
    :param data: DataFrame containing features and target ('Close').
    :param input_days: Number of past days to use as input.
    :param output_days: Number of future days to predict.
    :return: X, y (features and target arrays).
    """
    X, y = [], []
    for i in range(len(data) - input_days - output_days + 1):
        # Past `input_days` rows as input features
        X.append(data.iloc[i:i + input_days].values)
        # Next `output_days` rows as target
        y.append(data['Close'].iloc[i + input_days:i + input_days + output_days].values)
    return np.array(X), np.array(y)

# Prepare the dataset (ensure technical indicators are included)
# Features include Open, High, Low, Volume, and all added indicators
feature_columns = ['Open', 'High', 'Low', 'Volume', 'SMA_5', 'SMA_10', 'EMA_5', 'EMA_10', 'RSI_14']
data = df[feature_columns + ['Close']]  # Include target 'Close' for output

# Create sliding windows
input_days = 5  # Use past 5 days as input
output_days = 3  # Predict next 3 days
X, y = create_sliding_window(data, input_days, output_days)

# Verify shapes
print(f"Shape of X (features): {X.shape}")  # Should be (samples, input_days, features)
print(f"Shape of y (target): {y.shape}")    # Should be (samples, output_days)

from sklearn.model_selection import train_test_split

# Flatten X for splitting, later reshape it for models like LSTM
X_flattened = X.reshape(X.shape[0], -1)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_flattened, y, test_size=0.2, random_state=42)

# Reshape back after splitting for compatibility with models
X_train = X_train.reshape(X_train.shape[0], input_days, -1)
X_test = X_test.reshape(X_test.shape[0], input_days, -1)

print(f"Training set X: {X_train.shape}, y: {y_train.shape}")
print(f"Testing set X: {X_test.shape}, y: {y_test.shape}")

# Flatten X_train
X_train_flat = X_train.reshape(X_train.shape[0], -1)

# Now you can use it in your print statement
print(f"X_train_flat shape: {X_train_flat.shape}")  # Expected: (n_samples, n_features)
print(f"y_train shape: {y_train.shape}")           # Expected: (n_samples,)

print(f"X_train_flat shape: {X_train_flat.shape}")  # Expected: (n_samples, n_features)
print(f"y_train shape: {y_train.shape}")          # Expected: (n_samples,)

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Verify the splits
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

print(X[:5])
print(y[:5])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

y_train = y_train.ravel()
y_test = y_test.ravel()

print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

print(f"X_train_flat shape: {X_train_flat.shape}, y_train shape: {y_train.shape}")
print(f"X_test_flat shape: {X_test_flat.shape}, y_test shape: {y_test.shape}")

import numpy as np

# Convert the list to a NumPy array
new_data = np.array(new_data)

# Now you can reshape it
new_data_flat = new_data.reshape(new_data.shape[0], -1)

# Continue with the rest of your code
print(f"Reshaped new_data shape: {new_data_flat.shape}")

new_data = new_data.reshape(new_data.shape[0], -1)  # Flatten the data

print(f"X_train_flat shape: {X_train_flat.shape}")  # Expected: (n_samples, n_features)
print(f"y_train shape: {y_train.shape}")          # Expected: (n_samples,)

y_train = y_train[:X_train_flat.shape[0]]  # Adjust size if y_train is larger

print(f"X_train_flat shape: {X_train_flat.shape}")
print(f"y_train shape: {y_train.shape}")

model.fit(X_train_flat, y_train)

print(new_data.shape)  # Should be (n_samples, 50) since the model expects 50 features

new_data_flat = new_data.reshape(1, -1)  # Flatten to (1, 50)

import numpy as np

# Pad new_data to match the shape (1, 50)
new_data_padded = np.zeros((1, 50))
new_data_padded[0, :4] = new_data  # Insert the available features

print("New Data Shape:", new_data_padded.shape)
predictions = model.predict(new_data_padded)
print("Predictions:", predictions)

predictions = model.predict(new_data_padded)
print("Predictions:", predictions)

print("Input to model.predict:", new_data_padded)
print("Shape of Input:", new_data_padded.shape)

predictions = model.predict(new_data_padded)

new_data = new_data_padded  # Replace with correctly shaped data

print("Final Input Shape:", new_data_padded.shape)
predictions = model.predict(new_data_padded)
print("Predictions:", predictions)

predictions = model.predict(new_data)

# Example new inputs
new_data_1 = np.random.rand(1, 50)  # Replace with realistic inputs
# Replace ... with actual values
# Example: Random data with 50 features
new_data_2 = np.random.rand(1, 50)
  # Ensure there are 50 elements in total
     # Your actual test case

# Predictions
pred_1 = model.predict(new_data_1)
pred_2 = model.predict(new_data_2)

print(f"Prediction for new_data_1: {pred_1}")
print(f"Prediction for new_data_2: {pred_2}")

import matplotlib.pyplot as plt
import numpy as np

# Assuming y_test and y_test_pred are NumPy arrays
# If they are lists, convert them to NumPy arrays:
# y_test = np.array(y_test)
# y_test_pred = np.array(y_test_pred)

# Ensure y_test and y_test_pred have the same length
min_length = min(len(y_test), len(y_test_pred))
y_test_trimmed = y_test[:min_length]
y_test_pred_trimmed = y_test_pred[:min_length]

# Calculate residuals
residuals = y_test_trimmed - y_test_pred_trimmed

# Plot the histogram
plt.hist(residuals, bins=20, edgecolor='k')
plt.title("Residual Distribution")
plt.xlabel("Residual")
plt.ylabel("Frequency")
plt.show()

import joblib

# Save the model
joblib.dump(model, 'linear_regression_model.pkl')

# Load the model later
loaded_model = joblib.load('linear_regression_model.pkl')

import pickle

# Save the model to a file
with open("linear_model.pkl", "wb") as file:
    pickle.dump(model, file)

print("Model saved as 'linear_model.pkl'")

import joblib
from os import path

joblib.dump(model, "linear_model.joblib")
file_size = path.getsize("linear_model.joblib") / (1024 * 1024)  # Convert to MB
print(f"Model file size: {file_size:.2f} MB")

import pickle

# Save the model
with open("linear_model.pkl", "wb") as f:
    pickle.dump(model, f)

# Load the model
with open("linear_model.pkl", "rb") as f:
    loaded_model = pickle.load(f)

# Test the loaded model
print(loaded_model)

from joblib import dump, load

# Save model
dump(model, "linear_model.joblib")

# Load model
loaded_model = load("linear_model.joblib")

!pip install fastapi[all]

from fastapi import FastAPI
from pydantic import BaseModel
import pickle

app = FastAPI()
model = pickle.load(open('linear_model.pkl', 'rb'))  # Load the model

class Data(BaseModel):
    features: list

@app.post("/predict/")
def predict(data: Data):
    prediction = model.predict([data.features])
    return {"prediction": prediction.tolist()}

pip install streamlit pyngrok

!ngrok authtoken 2txGOxSXzB9a3X1P1trr0uQwMa5_cKRpUX8HNQMBcG4hyQUU

import streamlit as st
import subprocess
import sys
from pyngrok import ngrok
import joblib

# ... (Your Streamlit code) ...

# Run Streamlit with ngrok
ngrok.kill()
public_url = ngrok.connect(8501).public_url
print(f"Streamlit App URL: {public_url}")
process = subprocess.Popen([sys.executable, "-m", "streamlit", "run", sys.argv[0], "--server.port", "8501"])

# Simple Streamlit app to test
st.title("Streamlit and ngrok Test")
st.write("Hello, this is a test Streamlit app to check if ngrok works!")

import streamlit as st
import pickle

model = pickle.load(open('linear_model.pkl', 'rb'))  # Load the model

st.title('Prediction Web App')
features = st.text_input('Enter the features (comma-separated)')

if features:
    features = [float(x) for x in features.split(',')]
    prediction = model.predict([features])
    st.write(f'Prediction: {prediction[0]}')